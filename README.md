# Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks

This repo's contents allow to reproduce the experiments of the "Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks" paper, under review at NeurIPS 2023.

# Citation

Omitted for anonymity during the review process.

# Source code credit

The extraction of the MNIST dataset we used to conduct our experiments is provided (see pickle files). 

We thank the authors of the github repository [autograd hacks] (https://github.com/cybertronai/autograd-hacks) whose code helped us extracting the per-sample gradients in PyTorch.

# Reproducing the experiments

Our experiments run with Pytorch 1.9.1. To reproduce, run the file optimizers_MNIST.py.

# Licenses

The MNIST dataset is distributed under the GPL v3 license. Our work is distributed under the MIT license.

